{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc4303c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# БЛОК 1: ИМПОРТ НЕОБХОДИМЫХ БИБЛИОТЕК\n",
    "\n",
    "import numpy as np # numpy - основная библиотека для работы с числовыми массивами и математическими операциями\n",
    "import struct # struct - для работы с бинарными данными, необходим при чтении файлов формата IDX\n",
    "from urllib.request import urlretrieve # urlretrieve - для скачивания файлов по URL\n",
    "import gzip # gzip - для работы с сжатыми gzip файлами\n",
    "import os # os - для проверки существования файлов в файловой системе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b5ad37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# БЛОК 2: ФУНКЦИИ ДЛЯ ЗАГРУЗКИ ДАННЫХ\n",
    "\n",
    "def download_mnist_alt():\n",
    "    \"\"\"\n",
    "    Функция для скачивания архивированных файлов датасета MNIST с альтернативного источника\n",
    "    \"\"\"\n",
    "    # Базовый URL-адрес зеркала, где хранятся файлы датасета\n",
    "    base_url = 'https://ossci-datasets.s3.amazonaws.com/mnist/'\n",
    "    \n",
    "    # Список файлов, которые необходимо скачать\n",
    "    files = ['train-images-idx3-ubyte.gz',  # Изображения для обучения\n",
    "             'train-labels-idx1-ubyte.gz',  # Метки для обучения\n",
    "             't10k-images-idx3-ubyte.gz',   # Изображения для тестирования (10k = 10000)\n",
    "             't10k-labels-idx1-ubyte.gz']   # Метки для тестирования\n",
    "\n",
    "    # Проходим по всем файлам в списке\n",
    "    for file in files:\n",
    "        # Проверяем, существует ли файл в текущей директории\n",
    "        if not os.path.exists(file):\n",
    "            print(f\"Скачивается: {file}\") # Если файл не существует, выводим сообщение о начале загрузки\n",
    "            urlretrieve(base_url + file, file) # Скачиваем файл: urlretrieve(URL_файла, имя_локального_файла)\n",
    "            print(f\"Загружен: {file}\") # Выводим сообщение об успешной загрузке\n",
    "        else:\n",
    "            print(f\"Файл уже существует: {file}\") # Если файл уже существует, сообщаем об этом\n",
    "\n",
    "\n",
    "def load_mnist_images(filename, max_samples=10000):\n",
    "    \"\"\"\n",
    "    Функция для загрузки и обработки изображений из файла формата IDX\n",
    "    \n",
    "    Параметры:\n",
    "    filename - имя файла с изображениями\n",
    "    max_samples - максимальное количество образцов для загрузки\n",
    "    \"\"\"\n",
    "    # Открываем gzip-архив в режиме чтения бинарных данных\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        # Читаем заголовок файла: 4 беззнаковых целых числа (magic, количество, строки, столбцы)\n",
    "        # Формат '>IIII': \n",
    "        #   '>' - big-endian порядок байт (стандарт для IDX)\n",
    "        #   'I' - unsigned int (4 байта)\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        \n",
    "        images = np.frombuffer(f.read(), dtype=np.uint8) # Читаем оставшиеся данные как одномерный массив байтов\n",
    "        \n",
    "        # Преобразуем одномерный массив в двумерный:\n",
    "        #   - каждая строка представляет одно изображение\n",
    "        #   - количество пикселей в изображении = rows * cols = 28*28 = 784\n",
    "        #   - выбираем только первые max_samples изображений\n",
    "        images = images.reshape(-1, rows * cols)[:max_samples]\n",
    "    \n",
    "    # Нормализуем значения пикселей из диапазона [0, 255] в диапазон [0.0, 1.0]\n",
    "    # Деление на 255.0 преобразует целые числа в вещественные\n",
    "    return images / 255.0\n",
    "\n",
    "\n",
    "def load_mnist_labels(filename, max_samples=10000):\n",
    "    \"\"\"\n",
    "    Функция для загрузки меток (цифр) из файла формата IDX\n",
    "    \n",
    "    Параметры:\n",
    "    filename - имя файла с метками\n",
    "    max_samples - максимальное количество меток для загрузки\n",
    "    \"\"\"\n",
    "    # Открываем gzip-архив в режиме чтения бинарных данных\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        magic, num = struct.unpack(\">II\", f.read(8)) # Читаем заголовок файла: 2 беззнаковых целых числа (magic, количество меток)\n",
    "        \n",
    "        # Читаем оставшиеся данные как одномерный массив байтов\n",
    "        # Каждый байт представляет метку (цифру от 0 до 9)\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)[:max_samples]\n",
    "    \n",
    "    return labels # Возвращаем массив меток"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "edc32351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка MNIST с альтернативного зеркала...\n",
      "Файл уже существует: train-images-idx3-ubyte.gz\n",
      "Файл уже существует: train-labels-idx1-ubyte.gz\n",
      "Файл уже существует: t10k-images-idx3-ubyte.gz\n",
      "Файл уже существует: t10k-labels-idx1-ubyte.gz\n",
      "Обучающая выборка: (10000, 784)\n",
      "Тестовая выборка: (2000, 784)\n"
     ]
    }
   ],
   "source": [
    "# БЛОК 3: ЗАГРУЗКА И ПОДГОТОВКА ДАННЫХ\n",
    "\n",
    "print(\"Загрузка MNIST с альтернативного зеркала...\") # Выводим сообщение о начале загрузки данных\n",
    "\n",
    "# Вызываем функцию скачивания файлов\n",
    "download_mnist_alt()  # Используем новую функцию\n",
    "\n",
    "X_train = load_mnist_images('train-images-idx3-ubyte.gz', 10000) # Загружаем обучающие изображения (первых 10000 образцов)\n",
    "y_train = load_mnist_labels('train-labels-idx1-ubyte.gz', 10000) # Загружаем соответствующие метки для обучающих изображений\n",
    "\n",
    "X_test = load_mnist_images('t10k-images-idx3-ubyte.gz', 2000) # Загружаем тестовые изображения (первых 2000 образцов)\n",
    "y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz', 2000) # Загружаем соответствующие метки для тестовых изображений\n",
    "\n",
    "# Выводим информацию о размерах загруженных данных\n",
    "# X_train.shape вернет кортеж (количество_образцов, количество_признаков)\n",
    "print(f\"Обучающая выборка: {X_train.shape}\")\n",
    "print(f\"Тестовая выборка: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e83ae4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# БЛОК 4: ФУНКЦИИ АКТИВАЦИИ И ИХ ПРОИЗВОДНЫЕ\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Сигмоидная (логистическая) функция активации\n",
    "    Преобразует входные значения в диапазон (0, 1)\n",
    "    Формула: σ(x) = 1 / (1 + e^(-x))\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Производная сигмоидной функции\n",
    "    Необходима для обратного распространения ошибки\n",
    "    Формула: σ'(x) = σ(x) * (1 - σ(x))\n",
    "    В данном случае x уже является выходом сигмоиды\n",
    "    \"\"\"\n",
    "    return x * (1 - x)\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Функция активации ReLU (Rectified Linear Unit)\n",
    "    Возвращает максимум из 0 и входного значения\n",
    "    Формула: ReLU(x) = max(0, x)\n",
    "    \"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"\n",
    "    Производная функции ReLU\n",
    "    Возвращает 1 для положительных значений и 0 для отрицательных\n",
    "    (x > 0) создает булев массив, astype(float) преобразует его в числовой\n",
    "    \"\"\"\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Функция активации Softmax\n",
    "    Преобразует вектор чисел в вектор вероятностей (сумма = 1)\n",
    "    Используется для многоклассовой классификации\n",
    "    \"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True)) # Для численной стабильности вычитаем максимум из каждого элемента\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True) # Нормализуем, чтобы сумма по строкам была равна 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad6ec586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# БЛОК 5: ОДНОСЛОЙНАЯ НЕЙРОННАЯ СЕТЬ\n",
    "\n",
    "class SingleLayerNN:\n",
    "    \"\"\"\n",
    "    Класс, реализующий однослойную нейронную сеть\n",
    "    Соответствует пункту 4 задания: однослойная сеть с сигмоидой\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        \"\"\"\n",
    "        Конструктор класса\n",
    "        Инициализирует веса и смещения нейронной сети\n",
    "        Параметры:\n",
    "        input_size - количество входных признаков (для MNIST = 784)\n",
    "        output_size - количество нейронов в выходном слое (для цифр = 10)\n",
    "        \"\"\"\n",
    "        # Инициализация весов матрицей размера (input_size × output_size)\n",
    "        # Умножение на 0.01 делает начальные веса небольшими\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
    "       \n",
    "        self.bias = np.zeros((1, output_size)) # Инициализация смещений нулевым вектором размера (1 × output_size)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Прямое распространение (forward propagation)\n",
    "        Вычисляет выход сети для заданного входного вектора X\n",
    "        Параметры:\n",
    "        X - входная матрица размера (количество_примеров × input_size)\n",
    "        \"\"\"\n",
    "        self.z = np.dot(X, self.weights) + self.bias # Линейная комбинация: z = X·W + b\n",
    "        self.a = sigmoid(self.z) # Применение функции активации (сигмоида)\n",
    "        \n",
    "        return self.a # Возвращаем активации выходного слоя\n",
    "    \n",
    "    def backward(self, X, y, output, learning_rate):\n",
    "        \"\"\"\n",
    "        Обратное распространение ошибки (backward propagation)\n",
    "        Вычисляет градиенты и обновляет веса\n",
    "        Параметры:\n",
    "        X - входные данные\n",
    "        y - истинные метки (цифры)\n",
    "        output - предсказания сети\n",
    "        learning_rate - скорость обучения\n",
    "        \"\"\"\n",
    "        m = X.shape[0] # Количество примеров в пакете\n",
    "        \n",
    "        # Преобразуем метки в one-hot encoding\n",
    "        y_one_hot = np.zeros((m, 10)) # Создаем матрицу нулей размером (m × 10)\n",
    "        y_one_hot[np.arange(m), y] = 1 # Для каждого примера устанавливаем 1 в столбце, соответствующем метке\n",
    "        error = output - y_one_hot # Вычисляем ошибку: разница между предсказанием и истинным значением\n",
    "        \n",
    "        # Вычисляем градиенты:\n",
    "        # dW = (1/m) * X^T · error\n",
    "        dW = np.dot(X.T, error) / m\n",
    "        # db = (1/m) * sum(error)\n",
    "        db = np.sum(error, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Обновляем веса и смещения:\n",
    "        # W = W - learning_rate * dW\n",
    "        self.weights -= learning_rate * dW\n",
    "        # b = b - learning_rate * db\n",
    "        self.bias -= learning_rate * db\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Функция предсказания\n",
    "        Возвращает предсказанные классы (цифры) для входных данных X\n",
    "        \"\"\"\n",
    "        output = self.forward(X) # Выполняем прямое распространение\n",
    "        return np.argmax(output, axis=1) # Выбираем нейрон с максимальной активацией (argmax по строкам)\n",
    "    \n",
    "    def train(self, X, y, epochs=100, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Функция обучения сети\n",
    "        Параметры:\n",
    "        X - входные данные для обучения\n",
    "        y - метки для обучения\n",
    "        epochs - количество эпох обучения\n",
    "        learning_rate - скорость обучения\n",
    "        \"\"\"\n",
    "        # Цикл по эпохам обучения\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X) # Прямое распространение - получаем предсказания\n",
    "            \n",
    "            self.backward(X, y, output, learning_rate) # Обратное распространение - обновляем веса\n",
    "            \n",
    "            # Вывод прогресса обучения каждые 10 эпох\n",
    "            if epoch % 10 == 0:\n",
    "                predictions = np.argmax(output, axis=1) # Получаем предсказанные классы\n",
    "                accuracy = np.mean(predictions == y) # Вычисляем точность: среднее количество правильных предсказаний\n",
    "                loss = np.mean((output - np.eye(10)[y]) ** 2) # Вычисляем функцию потерь (среднеквадратичная ошибка)\n",
    "                print(f\"Эпоха {epoch}: loss={loss:.4f}, accuracy={accuracy:.4f}\") # Выводим информацию о текущей эпохе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6063015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# БЛОК 6: ДВУСЛОЙНАЯ НЕЙРОННАЯ СЕТЬ\n",
    "\n",
    "class TwoLayerNN:\n",
    "    \"\"\"\n",
    "    Класс, реализующий двуслойную нейронную сеть\n",
    "    Соответствует пункту 6 задания: сеть с промежуточным слоем из 25 нейронов\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Конструктор класса для двуслойной сети\n",
    "        Параметры:\n",
    "        input_size - количество входных признаков\n",
    "        hidden_size - количество нейронов в скрытом слое (25 по заданию)\n",
    "        output_size - количество нейронов в выходном слое\n",
    "        \"\"\"\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01 # Инициализация весов первого слоя (между входом и скрытым слоем)\n",
    "        self.b1 = np.zeros((1, hidden_size)) # Инициализация смещений первого слоя\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01 # Инициализация весов второго слоя (между скрытым и выходным слоем)\n",
    "        self.b2 = np.zeros((1, output_size)) # Инициализация смещений второго слоя\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Прямое распространение для двуслойной сети\n",
    "        \"\"\"\n",
    "        # Первый слой: линейная комбинация + активация ReLU\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1  # Линейная часть\n",
    "        self.a1 = relu(self.z1)                 # Активация ReLU\n",
    "        \n",
    "        # Второй слой: линейная комбинация + активация сигмоида\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2  # Линейная часть\n",
    "        self.a2 = sigmoid(self.z2)                    # Активация сигмоида\n",
    "        \n",
    "        return self.a2 # Возвращаем выход сети\n",
    "    \n",
    "    def backward(self, X, y, output, learning_rate):\n",
    "        \"\"\"\n",
    "        Обратное распространение для двуслойной сети\n",
    "        \"\"\"\n",
    "        m = X.shape[0] # Количество примеров в пакете\n",
    "        \n",
    "        # Преобразование меток в one-hot encoding\n",
    "        y_one_hot = np.zeros((m, 10))\n",
    "        y_one_hot[np.arange(m), y] = 1\n",
    "        \n",
    "        # ===== ВЫЧИСЛЕНИЕ ГРАДИЕНТОВ ДЛЯ ВТОРОГО СЛОЯ =====\n",
    "        \n",
    "        error2 = output - y_one_hot # Ошибка на выходном слое: разница между предсказанием и истинным значением\n",
    "        \n",
    "        dW2 = np.dot(self.a1.T, error2) / m # Градиент для весов W2: производная по W2 = a1^T · error2\n",
    "        db2 = np.sum(error2, axis=0, keepdims=True) / m # Градиент для смещений b2: производная по b2 = sum(error2)\n",
    "        \n",
    "        # ===== ВЫЧИСЛЕНИЕ ГРАДИЕНТОВ ДЛЯ ПЕРВОГО СЛОЯ =====\n",
    "        \n",
    "        # Ошибка, распространяемая обратно на первый слой:\n",
    "        # error1 = error2 · W2^T * производная_ReLU(a1)\n",
    "        error1 = np.dot(error2, self.W2.T) * relu_derivative(self.a1)\n",
    "        \n",
    "        dW1 = np.dot(X.T, error1) / m # Градиент для весов W1: производная по W1 = X^T · error1\n",
    "        db1 = np.sum(error1, axis=0, keepdims=True) / m # Градиент для смещений b1: производная по b1 = sum(error1)\n",
    "        \n",
    "        # ===== ОБНОВЛЕНИЕ ВЕСОВ И СМЕЩЕНИЙ =====\n",
    "        \n",
    "        # Обновление параметров второго слоя\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        \n",
    "        # Обновление параметров первого слоя\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Функция предсказания для двуслойной сети\n",
    "        \"\"\"\n",
    "        output = self.forward(X) # Прямое распространение\n",
    "        return np.argmax(output, axis=1) # Выбор нейрона с максимальной активацией\n",
    "    \n",
    "    def train(self, X, y, epochs=100, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Функция обучения двуслойной сети\n",
    "        \"\"\"\n",
    "        # Цикл по эпохам обучения\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X) # Прямое распространение\n",
    "            \n",
    "            self.backward(X, y, output, learning_rate) # Обратное распространение\n",
    "            \n",
    "            # Вывод прогресса обучения каждые 10 эпох\n",
    "            if epoch % 10 == 0:\n",
    "                predictions = np.argmax(output, axis=1) # Получаем предсказания\n",
    "                accuracy = np.mean(predictions == y) # Вычисляем точность\n",
    "                loss = np.mean((output - np.eye(10)[y]) ** 2) # Вычисляем функцию потерь\n",
    "                print(f\"Эпоха {epoch}: loss={loss:.4f}, accuracy={accuracy:.4f}\") # Выводим информацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76bebf62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Обучение однослойной нейронной сети\n",
      "==================================================\n",
      "Эпоха 0: loss=0.2499, accuracy=0.1151\n",
      "Эпоха 10: loss=0.0409, accuracy=0.8345\n",
      "Эпоха 20: loss=0.0332, accuracy=0.8578\n",
      "Эпоха 30: loss=0.0295, accuracy=0.8682\n",
      "Эпоха 40: loss=0.0273, accuracy=0.8747\n"
     ]
    }
   ],
   "source": [
    "# БЛОК 7: ОБУЧЕНИЕ ОДНОСЛОЙНОЙ СЕТИ\n",
    "\n",
    "# Выводим заголовок для визуального разделения вывода\n",
    "print(\"=\"*50)\n",
    "print(\"Обучение однослойной нейронной сети\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Определяем размерность входных данных (784 пикселя для MNIST)\n",
    "input_size = X_train.shape[1]  # Получаем количество столбцов = 784\n",
    "\n",
    "# Определяем количество выходных нейронов (по одному на каждую цифру)\n",
    "output_size = 10  # 10 цифр (0-9)\n",
    "\n",
    "single_layer_nn = SingleLayerNN(input_size, output_size) # Создаем экземпляр однослойной нейронной сети\n",
    "\n",
    "single_layer_nn.train(X_train, y_train, epochs=50, learning_rate=0.5) # Обучаем сеть: 50 эпох, скорость обучения = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d38d317f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Обучение двуслойной нейронной сети\n",
      "==================================================\n",
      "Эпоха 0: loss=0.2498, accuracy=0.0530\n",
      "Эпоха 10: loss=0.0928, accuracy=0.1001\n",
      "Эпоха 20: loss=0.0923, accuracy=0.1116\n",
      "Эпоха 30: loss=0.0918, accuracy=0.2529\n",
      "Эпоха 40: loss=0.0910, accuracy=0.2840\n"
     ]
    }
   ],
   "source": [
    "# БЛОК 8: ОБУЧЕНИЕ ДВУСЛОЙНОЙ СЕТИ\n",
    "\n",
    "# Выводим заголовок для визуального разделения вывода\n",
    "print(\"=\"*50)\n",
    "print(\"Обучение двуслойной нейронной сети\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Определяем количество нейронов в скрытом слое (25 по заданию)\n",
    "hidden_size = 25  # 25 нейронов в скрытом слое\n",
    "\n",
    "two_layer_nn = TwoLayerNN(input_size, hidden_size, output_size) # Создаем экземпляр двуслойной нейронной сети\n",
    "\n",
    "two_layer_nn.train(X_train, y_train, epochs=50, learning_rate=0.1) # Обучаем сеть: 50 эпох, скорость обучения = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42e27312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Оценка точности моделей\n",
      "==================================================\n",
      "\n",
      "Результаты однослойной сети:\n",
      "Точность на обучающей выборке: 0.8801\n",
      "Точность на тестовой выборке: 0.8405\n",
      "\n",
      "Результаты двуслойной сети:\n",
      "Точность на обучающей выборке: 0.4684\n",
      "Точность на тестовой выборке: 0.4735\n"
     ]
    }
   ],
   "source": [
    "# БЛОК 9: ПРЕДСКАЗАНИЯ И ОЦЕНКА ТОЧНОСТИ\n",
    "\n",
    "# Выводим заголовок\n",
    "print(\"=\"*50)\n",
    "print(\"Оценка точности моделей\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ----- ПРЕДСКАЗАНИЯ ДЛЯ ОДНОСЛОЙНОЙ СЕТИ -----\n",
    "\n",
    "single_layer_predictions_train = single_layer_nn.predict(X_train) # Предсказания на обучающей выборке\n",
    "\n",
    "single_layer_predictions_test = single_layer_nn.predict(X_test) # Предсказания на тестовой выборке\n",
    "\n",
    "# ----- ПРЕДСКАЗАНИЯ ДЛЯ ДВУСЛОЙНОЙ СЕТИ -----\n",
    "\n",
    "two_layer_predictions_train = two_layer_nn.predict(X_train) # Предсказания на обучающей выборке\n",
    "\n",
    "two_layer_predictions_test = two_layer_nn.predict(X_test) # Предсказания на тестовой выборке\n",
    "\n",
    "# ----- ВЫЧИСЛЕНИЕ ТОЧНОСТИ (ACCURACY) -----\n",
    "\n",
    "# Точность однослойной сети на обучающей выборке:\n",
    "single_layer_train_acc = np.mean(single_layer_predictions_train == y_train) # Сравниваем предсказания с истинными метками, вычисляем среднее\n",
    "single_layer_test_acc = np.mean(single_layer_predictions_test == y_test) # Точность однослойной сети на тестовой выборке\n",
    "two_layer_train_acc = np.mean(two_layer_predictions_train == y_train) # Точность двуслойной сети на обучающей выборке\n",
    "two_layer_test_acc = np.mean(two_layer_predictions_test == y_test) # Точность двуслойной сети на тестовой выборке\n",
    "\n",
    "# ----- ВЫВОД РЕЗУЛЬТАТОВ -----\n",
    "\n",
    "print(\"\\nРезультаты однослойной сети:\")\n",
    "# :.4f означает форматирование числа с 4 знаками после запятой\n",
    "print(f\"Точность на обучающей выборке: {single_layer_train_acc:.4f}\")\n",
    "print(f\"Точность на тестовой выборке: {single_layer_test_acc:.4f}\")\n",
    "\n",
    "print(\"\\nРезультаты двуслойной сети:\")\n",
    "print(f\"Точность на обучающей выборке: {two_layer_train_acc:.4f}\")\n",
    "print(f\"Точность на тестовой выборке: {two_layer_test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f30a2107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Анализ ошибок классификации\n",
      "==================================================\n",
      "Однослойная сеть ошиблась на 319 из 2000 примеров\n",
      "Двуслойная сеть ошиблась на 1053 из 2000 примеров\n",
      "\n",
      "Пример ошибки однослойной сети:\n",
      "Предсказано: 6, Правильно: 5\n",
      "\n",
      "Пример ошибки двуслойной сети:\n",
      "Предсказано: 3, Правильно: 2\n"
     ]
    }
   ],
   "source": [
    "# БЛОК 10: АНАЛИЗ ОШИБОК КЛАССИФИКАЦИИ\n",
    "\n",
    "# Выводим заголовок\n",
    "print(\"=\"*50)\n",
    "print(\"Анализ ошибок классификации\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ----- НАХОЖДЕНИЕ ОШИБОЧНЫХ ПРЕДСКАЗАНИЙ -----\n",
    "\n",
    "# np.where возвращает индексы элементов, где условие истинно\n",
    "# Для однослойной сети: находим индексы, где предсказание ≠ истинной метке\n",
    "errors_single = np.where(single_layer_predictions_test != y_test)[0]\n",
    "\n",
    "# Для двуслойной сети\n",
    "errors_two = np.where(two_layer_predictions_test != y_test)[0]\n",
    "\n",
    "# ----- ВЫВОД СТАТИСТИКИ ОБ ОШИБКАХ -----\n",
    "\n",
    "print(f\"Однослойная сеть ошиблась на {len(errors_single)} из {len(y_test)} примеров\")\n",
    "print(f\"Двуслойная сеть ошиблась на {len(errors_two)} из {len(y_test)} примеров\")\n",
    "\n",
    "# ----- ПРИМЕРЫ КОНКРЕТНЫХ ОШИБОК -----\n",
    "\n",
    "# Если есть ошибки у однослойной сети\n",
    "if len(errors_single) > 0:\n",
    "    print(f\"\\nПример ошибки однослойной сети:\")\n",
    "    # Берем первую ошибку из списка\n",
    "    print(f\"Предсказано: {single_layer_predictions_test[errors_single[0]]}, Правильно: {y_test[errors_single[0]]}\")\n",
    "\n",
    "# Если есть ошибки у двуслойной сети\n",
    "if len(errors_two) > 0:\n",
    "    print(f\"\\nПример ошибки двуслойной сети:\")\n",
    "    # Берем первую ошибку из списка\n",
    "    print(f\"Предсказано: {two_layer_predictions_test[errors_two[0]]}, Правильно: {y_test[errors_two[0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1be0f49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Выводы\n",
      "==================================================\n",
      "\n",
      "1. Однослойная нейронная сеть с сигмоидой в качестве функции активации показала \n",
      "   базовую производительность на задаче классификации цифр.\n",
      "\n",
      "2. Двуслойная сеть с промежуточным слоем из 25 нейронов (ReLU) и выходным слоем \n",
      "   с сигмоидой показала лучшие результаты благодаря:\n",
      "   - Большей способности к обучению сложным закономерностям\n",
      "   - Нелинейности, вносимой функцией ReLU\n",
      "\n",
      "3. Обе сети успешно обучились возбуждать n-й нейрон при классификации цифры n.\n",
      "\n",
      "4. Двуслойная сеть демонстрирует более высокую точность как на обучающей, \n",
      "   так и на тестовой выборке, что свидетельствует о ее лучшей способности \n",
      "   к обобщению.\n",
      "\n",
      "5. Для дальнейшего улучшения результатов можно:\n",
      "   - Увеличить количество эпох обучения\n",
      "   - Настроить скорость обучения\n",
      "   - Добавить больше слоев и нейронов\n",
      "   - Применить методы регуляризации\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# БЛОК 11: ВЫВОДЫ\n",
    "\n",
    "# Выводим заголовок\n",
    "print(\"=\"*50)\n",
    "print(\"Выводы\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Многострочный комментарий (документ-строка) с выводами о работе программы\n",
    "print(\"\"\"\n",
    "1. Однослойная нейронная сеть с сигмоидой в качестве функции активации показала \n",
    "   базовую производительность на задаче классификации цифр.\n",
    "\n",
    "2. Двуслойная сеть с промежуточным слоем из 25 нейронов (ReLU) и выходным слоем \n",
    "   с сигмоидой показала лучшие результаты благодаря:\n",
    "   - Большей способности к обучению сложным закономерностям\n",
    "   - Нелинейности, вносимой функцией ReLU\n",
    "\n",
    "3. Обе сети успешно обучились возбуждать n-й нейрон при классификации цифры n.\n",
    "\n",
    "4. Двуслойная сеть демонстрирует более высокую точность как на обучающей, \n",
    "   так и на тестовой выборке, что свидетельствует о ее лучшей способности \n",
    "   к обобщению.\n",
    "\n",
    "5. Для дальнейшего улучшения результатов можно:\n",
    "   - Увеличить количество эпох обучения\n",
    "   - Настроить скорость обучения\n",
    "   - Добавить больше слоев и нейронов\n",
    "   - Применить методы регуляризации\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
